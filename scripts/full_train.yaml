runner: "episode_runner"
mac: "basic_mac"
learner: "q_learner"

t_max: 100000000   
test_interval: 0   
test_nepisode: 0

text_embed_dim: 1024
n_agents: 3
belief_dim: 128
batch_size_run: 1
state_shape: 1024

agent_output_type: "q_values"
action_selector: "multinomial"
use_causal_mask: false
max_seq_length: 1024
n_actions: 2

llm_model_name: "gpt2"
together_api_key: "${TOGETHER_API_KEY}"
coordinator_model: "meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo"
executor_model: "meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo"
commitment_embedding_model_name: "BAAI/bge-large-en-v1.5"

env: "huggingface_dataset_env"
env_args:
  hf_dataset_path: "gsm8k"
  hf_dataset_config_name: "main"
  dataset_split: "train"
  max_question_length: 1024
  max_answer_length: 200
  dataset_streaming: false
  use_random_sampling: false
  random_without_replacement: true
  loop_dataset: false       
  use_dataset_episode: false
  max_rounds: 3
  reward:
    initial_weights: [0.4, 0.4, 0.2]

lr: 0.001
belief_net_lr: 0.001
encoder_lr: 0.001
mixer_lr: 0.001
weight_decay: 0.0
episode_length: 3

bne_max_iterations: 3
bne_convergence_threshold: 0.01
stage2_weight: 0.3

arch:
  entity_dim: 128
  attention_heads: 4
  transformer_blocks: 2
  key_dim: 32
  mlp_hidden_size: 128
  feedforward_size: 512
  dropout_rate: 0.1
  layer_norm_epsilon: 0.00001

prompt_attention_heads: 2
commitment_embedding_dim: 1024

sampling:
  temperature_min: 0.1
  temperature_max: 2.0
  p_min: 0.1
  p_max: 0.9

reward:
  max_value: 1.0
  initial_weights: [0.4, 0.4, 0.2]
  eta_alpha: 0.001
  dynamic_alpha_update: true
  r_expected_source: "q_value"
  al_weight: 0.2
  ts_weight: 0.6
  cc_weight: 0.2

loss:
  belief_weight: 0.1
  encoder_weight: 0.1
  mixing_weight: 0.1

early_stopping:
  commitment_threshold: 0.05
  loss_threshold: 0.001
  reward_threshold: 0.5
  patience: 3
  min_delta: 0.0
  warmup: 0

system:
  use_cuda: true
  device_num: 0
  seed: 42
  debug: true

logging:
  use_tensorboard: true
  log_interval: 50
  save_model: true
  save_model_interval: 2000
  checkpoint_path: "./models_full"
  log_path: "./logs_full"
  experiment_name: "econ_full_pass"

llm:
  together_api_key: "${TOGETHER_API_KEY}"
  coordinator_model: "meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo"
  executor_model: "meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo"
  max_tokens: 2048

use_replay: false
